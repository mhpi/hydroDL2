{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " configuration found: conf/model_config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import bmi_dpl_model as bmi_model  # This is the BMI we will test.\n",
    "\n",
    "# Define config path\n",
    "cfg_file=Path('./conf/model_config.yaml')\n",
    "\n",
    "if os.path.exists(cfg_file):\n",
    "    print(\" configuration found: \" + str(cfg_file))\n",
    "else:\n",
    "    print(\" no configuration found, exiting...\")\n",
    "    sys.exit()\n",
    "\n",
    "bmi=bmi_model.BMIdPLModel(cfg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ensemble_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbmi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lgl5139/ngen/extern/dpl_model_package/bmi_dpl_model.py:316\u001b[0m, in \u001b[0;36mBMIdPLModel.initialize\u001b[0;34m(self, config_filepath)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_step_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_step_delta\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Load a trained model.\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43mModelHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward_init\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# Forward model on all data in this .initialize() step.\u001b[39;00m\n",
      "File \u001b[0;32m/data/lgl5139/hydro_multimodel/dPLHydro_multimodel/models/model_handler.py:30\u001b[0m, in \u001b[0;36mModelHandler.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDifferentiable Model Handler\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lgl5139/hydro_multimodel/dPLHydro_multimodel/models/model_handler.py:39\u001b[0m, in \u001b[0;36mModelHandler._init_models\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mInitialize and store each differentiable hydro model and optimizer in\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mthe multimodel.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mensemble_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhydro_models\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiple hydro models given, but ensemble type not specified. Check config.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_wnn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Reinitialize trained models for wNN training.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ensemble_type'"
     ]
    }
   ],
   "source": [
    "bmi.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # debugging ----- #\n",
    "    n_timesteps, ngrid = dataset_dict['inputs_nn_scaled'].shape\n",
    "    print(n_timesteps, ngrid)\n",
    "    # n_timesteps = 400\n",
    "    # n_basins = 671\n",
    "    n_basins = dataset_dict['c_nn'].shape[0] \n",
    "    print(n_basins)\n",
    "    # --------------- #\n",
    "\n",
    "    # Store attributes and forcings in BMI if end_timestep > 100 days.\n",
    "    # NOTE: maybe data gets passed in this step no matter what, but for <100 day\n",
    "    # periods, it's only seen by dPL model on update call.\n",
    "    # n_timesteps = model.config['end_timestep']\n",
    "    rho = model.config['rho']  # For routing\n",
    "\n",
    "    i_start = np.arange(0, ngrid, model.config['batch_basins'])\n",
    "    i_end = np.append(i_start[1:], ngrid)\n",
    "    \n",
    "    # if n_timesteps > 100:\n",
    "    for t in range(len(i_start)):\n",
    "        # NOTE: for each timestep in this loop, the data assignments below are of\n",
    "        # arrays of basins. e.g., forcings['key'].shape = (rho + 1, # basins).\n",
    "        # TODO: add basin batching from pmi.train.py experiment for this loop instead\n",
    "        # of doing all basins at once.\n",
    "\n",
    "        # Forward through basins in batches.\n",
    "        dataset_dict_sample = take_sample_test(model.config, dataset_dict,\n",
    "                                               i_start[i], i_end[i])\n",
    "        ################## Map forcings + attributes into BMI ##################\n",
    "        # Set NN forcings...\n",
    "        for i, var in enumerate(model.config['observations']['var_t_nn']):\n",
    "            standard_name = model._var_name_map_short_first[var]\n",
    "            model.set_value(standard_name, dataset_dict_sample['inputs_nn_scaled'][t:rho + t + 1, :n_basins, i], model='nn')\n",
    "        n_forc = i\n",
    "        \n",
    "        # Set NN attributes...\n",
    "        for i, var in enumerate(model.config['observations']['var_c_nn']):\n",
    "            standard_name = model._var_name_map_short_first[var]\n",
    "            model.set_value(standard_name, dataset_dict_sample['inputs_nn_scaled'][t:rho + t + 1, :n_basins, n_forc + i + 1], model='nn') \n",
    "\n",
    "        # Set physics model forcings...\n",
    "        for i, var in enumerate(model.config['observations']['var_t_hydro_model']):\n",
    "            standard_name = model._var_name_map_short_first[var]\n",
    "            model.set_value(standard_name, dataset_dict_sample['x_hydro_model'][t:rho + t + 1, :n_basins, i], model='pm') \n",
    "\n",
    "        # Set physics model attributes...\n",
    "        for i, var in enumerate(model.config['observations']['var_c_hydro_model']):\n",
    "            standard_name = model._var_name_map_short_first[var]\n",
    "            # NOTE: These don't have a time dimension.\n",
    "            model.set_value(standard_name, dataset_dict_samples['c_hydro_model'][:n_basins, i], model='pm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        ngrid = dataset_dict['inputs_nn_scaled'].shape[1]\n",
    "        self.iS = np.arange(0, ngrid, self.config['batch_basins'])\n",
    "        self.iE = np.append(self.iS[1:], ngrid)\n",
    "\n",
    "    def _get_model_predictions(self) -> List[Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Get predictions from a trained model.\n",
    "        \"\"\"\n",
    "        batched_preds_list = []\n",
    "        for i in tqdm.tqdm(range(len(self.iS)), leave=False, dynamic_ncols=True):\n",
    "            dataset_dict_sample = take_sample_test(self.config, self.dataset_dict,\n",
    "                                                   self.iS[i], self.iE[i])\n",
    "            # Forward pass for hydrology models.\n",
    "            hydro_preds = self.dplh_model_handler(dataset_dict_sample, eval=True)\n",
    "\n",
    "            # Compile predictions from each batch.\n",
    "            if self.config['ensemble_type'] in ['frozen_pnn', 'free_pnn']:\n",
    "                # For ensembles w/ wNN: Forward pass for wNN to get ensemble weights.\n",
    "                self.ensemble_lstm(dataset_dict_sample, eval=True)\n",
    "\n",
    "                # Ensemble hydrology models using learned weights.\n",
    "                ensemble_pred = self.ensemble_lstm.ensemble_models(hydro_preds)\n",
    "                batched_preds_list.append({key: tensor.cpu().detach() for key,\n",
    "                                           tensor in ensemble_pred.items()})\n",
    "            elif self.config['ensemble_type'] == 'avg':\n",
    "                # For 'average' type ensemble: Average model predictions at each\n",
    "                # basin for each day.\n",
    "                ensemble_pred = model_average(hydro_preds, self.config)\n",
    "                batched_preds_list.append({key: tensor.cpu().detach() for key,\n",
    "                                           tensor in ensemble_pred.items()})\n",
    "            else:\n",
    "                # For single hydrology model.\n",
    "                model_name = self.config['hydro_models'][0]\n",
    "                batched_preds_list.append({key: tensor.cpu().detach() for key,\n",
    "                                           tensor in hydro_preds[model_name].items()})\n",
    "        return batched_preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Function\n",
    "\n",
    "class BMIWrapperFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_tensor):\n",
    "        # Convert PyTorch tensor to NumPy array\n",
    "        input_np = input_tensor.detach().cpu().numpy()\n",
    "\n",
    "        # Call the BMI model (assumed to be a method `run_bmi_model`)\n",
    "        # `run_bmi_model` is a placeholder for your BMI interface\n",
    "        output_np = run_bmi_model(input_np)  # Replace with actual BMI call\n",
    "\n",
    "        # Save the input for backward pass\n",
    "        ctx.save_for_backward(input_tensor)\n",
    "\n",
    "        # Convert output back to a PyTorch tensor\n",
    "        output_tensor = torch.from_numpy(output_np).to(input_tensor.device)\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve the saved tensor from forward pass\n",
    "        input_tensor, = ctx.saved_tensors\n",
    "\n",
    "        # Convert PyTorch tensor to NumPy array for BMI gradient computation\n",
    "        grad_output_np = grad_output.detach().cpu().numpy()\n",
    "\n",
    "        # Compute gradients using BMI model (replace with actual gradient call)\n",
    "        grad_input_np = compute_bmi_gradient(input_tensor.cpu().numpy(), grad_output_np)\n",
    "\n",
    "        # Convert gradients back to PyTorch tensor\n",
    "        grad_input_tensor = torch.from_numpy(grad_input_np).to(input_tensor.device)\n",
    "\n",
    "        return grad_input_tensor\n",
    "\n",
    "\n",
    "def run_bmi_model(input_np):\n",
    "    \"\"\"Placeholder for running the BMI model.\"\"\"\n",
    "    # Your logic for passing data through the BMI interface\n",
    "    # For example, bmi_model.update(input_np)\n",
    "    # Return the output NumPy array\n",
    "    return input_np * 2  # Just an example operation\n",
    "\n",
    "\n",
    "def compute_bmi_gradient(input_np, grad_output_np):\n",
    "    \"\"\"Placeholder for computing BMI model gradients.\"\"\"\n",
    "    # Your logic for computing the gradient\n",
    "    # You may need to implement this manually if BMI doesn't handle it\n",
    "    return grad_output_np  # Example, assuming identity gradient\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mulhydrodl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
